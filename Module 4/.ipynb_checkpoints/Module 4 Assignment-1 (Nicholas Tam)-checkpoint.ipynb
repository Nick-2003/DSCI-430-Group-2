{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0df0250-eb83-4221-8c6a-c068cf438e10",
   "metadata": {},
   "source": [
    "# Module 4 - Reducing unfairness in learning algorithm applications \n",
    "\n",
    "\n",
    "### Assignment overview\n",
    "\n",
    "In this assignment, you are tasked to create a classifer to predict the estimated income of individuals in the [Kaggle Adult Income Dataset](https://www.kaggle.com/datasets/wenruliu/adult-income-dataset). This dataset is known to be biased towards certain groups. You will try some strategies to create a more fair classifier.\n",
    "\n",
    "For this assignment, it is possible to work in **groups of up to 2 students**. Read the instructions carefully, as they may assign tasks to specific students.\n",
    "\n",
    "### Group members\n",
    "Leave blanks if group has less than 2 members:\n",
    "- Student 1: Jingyuan Liu (S.N. 69763183)\n",
    "- Student 2: Nicholas Tam (S.N. 45695970)\n",
    "\n",
    "\n",
    "### Learning Goals:\n",
    "\n",
    "After completing this week's lecture and tutorial work, you will be able to:\n",
    "1. Discuss the consequences of erroneous (biased) data on the training of learning algorithms and how it impacts its end users  \n",
    "2. Discuss potential ethical implications in errors in feature selection, model selection \n",
    "3. Describe strategies for reducing algorithmic bias \n",
    "4. Apply strategies to reduce unfairness in a predictive model trained on an unbalanced dataset \n",
    "5. Describe advantages and limitations of the strategies used to reduce unfairness in predictive models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af265b-c6aa-497a-8fb5-d14afeba29b7",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "Here are some libraries you will need for this assignment. `imblearn` and `aif360` are new ones, you can install it by running the cell below. Comment out this line after one execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fa076a-9816-406e-970d-03c0386acc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install imblearn \n",
    "# !pip install aif360 \n",
    "\n",
    "# !conda install imblearn -n DSCI430\n",
    "# !conda install aif360 -n DSCI430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8025138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61a1a6-53d6-411e-ae78-ce97137f07f3",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset you will use for this assignment is the [Kaggle Adult Income Dataset](https://www.kaggle.com/datasets/wenruliu/adult-income-dataset). You may visit the source page for more information about this dataset.\n",
    "\n",
    "The dataset includes 15 columns: 14 of them are demographics and other features to describe a person, and one (the target variable), is their income. The income variable is binary and has the two possible values `<=50K` or `>50K`.\n",
    "\n",
    "Let's start by importing the dataset and taking a look (you are free to add other lines if you want more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379b33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40dac5-6aae-448e-aa57-fb9bdc083f59",
   "metadata": {},
   "source": [
    "Unfortunately, this dataset is notoriously biased in the association between income and other demographic information, such as race and gender. Let's see how.\n",
    "\n",
    "#### Question 1 \n",
    "Create the following 3 bar charts:\n",
    "- A global bar chart of the target variable\n",
    "- A bar chart of the target variable divided by gender\n",
    "- A bar chart of the target variable divided by race\n",
    "\n",
    "Comment on the results. Is the target variable balanced? Is the target variable balanced across protected groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc68ca6-817e-4852-8555-1e56a45cd57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "\n",
    "display(df[\"income\"].value_counts(normalize=True).sort_index())\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "frame = df[\"income\"].astype(\"string\").value_counts().sort_index()\n",
    "plt.bar(frame.index, frame)\n",
    "plt.xlabel(\"Income category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of income\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116de032-bbe8-473f-ad55-25aa7d6f47d2",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Across the entire dataset, the target variable `income` is not balanced, with 0.760718 of the samples having `income == \"<=50K\"`, and 0.239282 of the samples having `income == \">50K\"`. As a result, the model may have biased predictive results in favor of the more frequent class of the target variable.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344208bb-7d80-48a9-a931-87f767a42189",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = df[\"gender\"].astype(\"string\").value_counts().sort_index().index\n",
    "\n",
    "for gen in genders: \n",
    "    data = df[df[\"gender\"] == gen]\n",
    "    print(\"Distribution of income for \" + gen.lower())\n",
    "    display(data[\"income\"].value_counts(normalize=True).sort_index())\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    frame = data[\"income\"].astype(\"string\")\n",
    "    f_cats = frame.value_counts().sort_index()\n",
    "    plt.bar(f_cats.index, f_cats)\n",
    "    plt.xlabel(gen)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of income for \" + gen.lower()) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba110021-f233-4c42-9c6d-d3578416ef65",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The target variable `income` is not balanced across each `gender` groups, nor are the distributions of each `income` category equal across each level of `gender`. For instance, the proportion of `income == \"<=50K\"` samples is 0.890749 among female individuals and 0.696233 among male individuals. This runs the risk of differential treatment and classification of `income` between categories of protected characteristics and increases the predictive bias against certain groups under protected characteristics.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47284808-705f-4625-a418-a1683781014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df[\"race\"].astype(\"string\").value_counts().sort_index().index\n",
    "\n",
    "for gen in races: \n",
    "    data = df[df[\"race\"] == gen]\n",
    "    print(\"Distribution of income for \" + gen.lower())\n",
    "    display(data[\"income\"].value_counts(normalize=True).sort_index())\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    frame = data[\"income\"].astype(\"string\")\n",
    "    f_cats = frame.value_counts().sort_index()\n",
    "    plt.bar(f_cats.index, f_cats)\n",
    "    plt.xlabel(gen)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of income for \" + gen.lower()) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0c9e5-1224-490f-be94-b6367dd662d5",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The target variable `income` is not balanced across each `race` groups, nor are the distributions of each `income` category equal across each level of `race`. For instance, the proportion of `income == \"<=50K\"` samples is 0.882979 among amer-indian-eskimo individuals and 0.746013 among white individuals. This runs the risk of differential treatment and classification of `income` between categories of protected characteristics and increases the predictive bias against certain groups under protected characteristics.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070e9db-576b-48fc-ab34-62a151ea6f13",
   "metadata": {},
   "source": [
    "### A biased classifier\n",
    "\n",
    "We can expect that a classifier trained on this kind of data will show some problematic behaviors when assigning an individual to a predicted income level. Let's visualize this using a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12f218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 1\n",
    "# Run this cell create training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=123)\n",
    "\n",
    "X_train, y_train = (\n",
    "    train_df.drop(columns=[\"income\"]),\n",
    "    train_df[\"income\"],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    test_df.drop(columns=[\"income\"]),\n",
    "    test_df[\"income\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc85e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 2\n",
    "# Run this cell to do the necessary dataset preprocessing (encoding of categorical features).\n",
    "# Note that, since we are using a tree based classifier, we don't need to scale the \n",
    "# numerical features.\n",
    "\n",
    "categorical_feats = [\"workclass\",\n",
    "                     \"education\",\n",
    "                     \"marital-status\",\n",
    "                     \"occupation\",\n",
    "                     \"relationship\",\n",
    "                     \"race\",\n",
    "                     \"gender\",\n",
    "                     \"native-country\",\n",
    "                     ]  # Apply one-hot encoding\n",
    "passthrough_feats = [\"age\",\n",
    "                \"fnlwgt\",\n",
    "                \"educational-num\",\n",
    "                \"capital-gain\",\n",
    "                \"capital-loss\",\n",
    "                \"hours-per-week\"\n",
    "                ]  # Numerical - no need to scale\n",
    "target = \"income\"\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (\n",
    "        make_pipeline(OneHotEncoder(handle_unknown=\"ignore\",drop=\"if_binary\")),\n",
    "        categorical_feats,\n",
    "    ),  # OHE on categorical features\n",
    "    (\"passthrough\", passthrough_feats)  # no transformations on numerical features\n",
    ")\n",
    "\n",
    "X_train_transformed = ct.fit_transform(X_train).toarray()\n",
    "\n",
    "column_names = list(\n",
    "    ct.named_transformers_[\"pipeline\"].get_feature_names_out(\n",
    "        categorical_feats\n",
    "    )\n",
    ") + passthrough_feats\n",
    "\n",
    "X_test_transformed = ct.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237ca8b-545e-4887-89c9-5cfe24864cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may use this lines to check the result\n",
    "pd.DataFrame(X_train_transformed, columns=column_names).head()\n",
    "# pd.DataFrame(X_test_transformed, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12bddc7-e7b2-4c6c-8210-5f43f898cc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 3\n",
    "# Run this cell to train a random forest classifer. The hyperparameters have been pre-selected\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, max_depth = 19, n_estimators = 100).fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e91002-096e-4778-8605-21b0befd709e",
   "metadata": {},
   "source": [
    "How good is this classifier? Let's check its accuracy, by running the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77345674-789f-4d25-805c-3430ea53173f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.score(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df66983-645b-46b2-a550-bd742d06ccf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de369cad-467e-4c4e-b71b-78a2cbcde10c",
   "metadata": {},
   "source": [
    "Finally, let's see what features are considered important by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b0ae6-5820-4ea5-b383-e145987781c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Sort the feature importances from greatest to least using the sorted indices\n",
    "sorted_indices = feature_importances.argsort()[::-1]\n",
    "sorted_feature_names = ct.get_feature_names_out()[sorted_indices]\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "\n",
    "# # Create a bar plot of the feature importances\n",
    "sns.set(rc={'figure.figsize':(11.7,30)})\n",
    "sns.barplot(x=sorted_importances, y=sorted_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe199c-fc3f-4d65-8b5b-27fbae0057b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a789447e-1295-4bb1-a9fb-02b7b493f217",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "What are the most important features for this classifier? Do they include protected characteristics, such as race or gender?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8747b-984b-4a74-943b-8703d765da1a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The top most important features for the classifier with scores greater than approximately 0.01 are `passthrough__capital-gain`, `pipeline__marital-status_Married-civ-spouse`, `passthrough__age`, `passthrough__educational-num`, `passthrough__hours-per-week`, `passthrough__fnlwgt`, `pipeline__relationship_Husband`, `pipeline__marital_status_Never-married`, `passthrough__capital-loss`, `passthrough__occupation_Exec-managerial`, `pipeline__occupation_Prof-specialty`, `pipeline__education_Bachelors`, `pipeline__gender_Male`, `pipeline__relationship_Not-in-family`, `pipeline__relationship_Own-child', 'pipeline__relationship_Wife`, `pipeline__marital-status_Divorced`, and `pipeline__education_Masters`. There are protected characteristics among the most important features, most notably `pipeline__marital-status_Married-civ-spouse`, `passthrough__age`, `pipeline__relationship_Husband`, and `pipeline__marital_status_Never-married`, `pipeline__gender_Male`, `pipeline__relationship_Not-in-family`, `pipeline__relationship_Own-child', 'pipeline__relationship_Wife`, and `pipeline__marital-status_Divorced`.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fa0ae-55d9-4506-aa9a-929b1dc1d07c",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "From Assignment 3, we have learned that a classifier may perform well in terms of accuracy, but being unfair to protected groups in the dataset. Use what you have learned in Assignment 3 and **evaluate this classifier for fairness in treating the two gender groups included in this dataset.** In particular, do the following: \n",
    "\n",
    "- Compute the 6 fairness metrics and the Average Distance from the Reference on training and test sets. You may reuse portions of code you have included in Assignment 3.\n",
    "- Comment on the results, providing an interpretation for each computed metric; how different is the treatment of the two groups? Is one (or more) of the metrics particularly concerning?\n",
    "\n",
    "Here is a recap of the fairness metrics:\n",
    "1. *Predicted Positive Rate Disparity (PPRD)*, whether the numbers of positive predictions are on par across groups.\n",
    "2. *Predicted Positive Group Rate Disparity (PPGRD)*, whether the rates of positive predictions are on par across groups.\n",
    "3. *False Discovery Rate Disparity (FDRD)*, whether the ratios of false positives to predicted positives are on par across groups.\n",
    "4. *False Positive Rate Disparity (FPRD)*, whether the ratios of false positives to actual negatives are on par across groups.\n",
    "5. *False Omission Rate Disparity (FORD)*, whether the ratios of false negatives to predicted negatives are on par across groups.\n",
    "6. *False Negative Rate Disparity (FNRD)*, whether the ratios of false negatives to actual positives are on par across groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a47ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer here (you may add more cells)\n",
    "# Add as many cells as needed to compute the required metrics for every classifier. You may\n",
    "# also add markdown cells if you want to add comments or notes about your results.\n",
    "\n",
    "# Splitting datasets by gender\n",
    "train_df_m = train_df[train_df[\"gender\"] == \"Male\"]\n",
    "train_df_f = train_df[train_df[\"gender\"] == \"Female\"]\n",
    "test_df_m = test_df[test_df[\"gender\"] == \"Male\"]\n",
    "test_df_f = test_df[test_df[\"gender\"] == \"Female\"]\n",
    "# train_df_m.head()\n",
    "\n",
    "# Creating training and test sets and separating features and target\n",
    "X_train_m, y_train_m = (\n",
    "    train_df_m.drop(columns=[\"income\"]),\n",
    "    train_df_m[\"income\"],\n",
    ")\n",
    "X_test_m, y_test_m = (\n",
    "    test_df_m.drop(columns=[\"income\"]),\n",
    "    test_df_m[\"income\"],\n",
    ")\n",
    "X_train_f, y_train_f = (\n",
    "    train_df_f.drop(columns=[\"income\"]),\n",
    "    train_df_f[\"income\"],\n",
    ")\n",
    "X_test_f, y_test_f = (\n",
    "    test_df_f.drop(columns=[\"income\"]),\n",
    "    test_df_f[\"income\"],\n",
    ")\n",
    "\n",
    "# xsets = [X_train_m, X_test_m, X_train_f, X_test_f]\n",
    "# ysets = [y_train_m, y_test_m, y_train_f, y_test_f]\n",
    "\n",
    "# Do not need to refit due to both ultimately training on the same set as a whole\n",
    "X_train_transformed_m = pd.DataFrame(ct.transform(X_train_m).toarray(), columns=column_names)\n",
    "X_train_transformed_f = pd.DataFrame(ct.transform(X_train_f).toarray(), columns=column_names)\n",
    "\n",
    "X_test_transformed_m = pd.DataFrame(ct.transform(X_test_m).toarray(), columns=column_names)\n",
    "X_test_transformed_f = pd.DataFrame(ct.transform(X_test_f).toarray(), columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61bc44-4c36-480c-9c0b-b704b7643ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_mets = {\n",
    "    \"model\": [],\n",
    "    \"PPRD\": [],\n",
    "    \"PPGRD\": [],\n",
    "    \"FDRD\": [],\n",
    "    \"FPRD\": [],\n",
    "    \"FORD\": [],\n",
    "    \"FNRD\": [],\n",
    "    # \"PPRD_adfr\": [],\n",
    "    # \"PPGRD_adfr\": [],\n",
    "    # \"FDRD_adfr\": [],\n",
    "    # \"FPRD_adfr\": [],\n",
    "    # \"FORD_adfr\": [],\n",
    "    # \"FNRD_adfr\": [],\n",
    "    \"adfr\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b55f1-bce6-412d-a8d3-99a50f47ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/28493/confusion-matrix-get-items-fp-fn-tp-tn-python\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def fairness_metrics(modelname, model, X_m, y_m, X_f, y_f, name):\n",
    "    cm_m = confusion_matrix(y_m, model.predict(X_m))\n",
    "    cm_f = confusion_matrix(y_f, model.predict(X_f))\n",
    "    \n",
    "    # TP_m = cm_m[0][0] \n",
    "    # FP_m = cm_m[0][1] \n",
    "    # FN_m = cm_m[1][0] \n",
    "    # TN_m = cm_m[1][1] \n",
    "\n",
    "    # TP_f = cm_f[0][0] \n",
    "    # FP_f = cm_f[0][1] \n",
    "    # FN_f = cm_f[1][0] \n",
    "    # TN_f = cm_f[1][1] \n",
    "\n",
    "    TP_m = cm_m[1][1] \n",
    "    FP_m = cm_m[0][1] \n",
    "    FN_m = cm_m[1][0] \n",
    "    TN_m = cm_m[0][0] \n",
    "\n",
    "    TP_f = cm_f[1][1] \n",
    "    FP_f = cm_f[0][1] \n",
    "    FN_f = cm_f[1][0] \n",
    "    TN_f = cm_f[0][0] \n",
    "\n",
    "    PPRD = (TP_m + FP_m)/(TP_f + FP_f)\n",
    "    PPGRD = ((TP_m + FP_m)/(TP_m + FP_m + FN_m + TN_m))/((TP_f + FP_f)/(TP_f + FP_f + FN_f + TN_f))\n",
    "    FDRD = (FP_m/(TP_m + FP_m))/(FP_f/(TP_f + FP_f))\n",
    "    FPRD = (FP_m/(TN_m + FP_m))/(FP_f/(TN_f + FP_f))\n",
    "    FORD = (FN_m/(TN_m + FN_m))/(FN_f/(TN_f + FN_f))\n",
    "    FNRD = (FN_m/(TP_m + FN_m))/(FN_f/(TP_f + FN_f))\n",
    "\n",
    "    PPRD_adfr = abs(PPRD - 1)\n",
    "    PPGRD_adfr = abs(PPGRD - 1)\n",
    "    FDRD_adfr = abs(FDRD - 1)\n",
    "    FPRD_adfr = abs(FPRD - 1)\n",
    "    FORD_adfr = abs(FORD - 1)\n",
    "    FNRD_adfr = abs(FNRD - 1)\n",
    "    adfr = np.mean([PPRD_adfr, PPGRD_adfr, FDRD_adfr, FPRD_adfr, FORD_adfr, FNRD_adfr])\n",
    "\n",
    "    fairness_mets[\"model\"].append(str(modelname) + \" (\" + str(name) + \")\")\n",
    "    fairness_mets[\"PPRD\"].append(PPRD)\n",
    "    fairness_mets[\"PPGRD\"].append(PPGRD)\n",
    "    fairness_mets[\"FDRD\"].append(FDRD)\n",
    "    fairness_mets[\"FPRD\"].append(FPRD)\n",
    "    fairness_mets[\"FORD\"].append(FORD)\n",
    "    fairness_mets[\"FNRD\"].append(FNRD)\n",
    "    # fairness_mets[\"PPRD_adfr\"].append(PPRD_adfr)\n",
    "    # fairness_mets[\"PPGRD_adfr\"].append(PPGRD_adfr)\n",
    "    # fairness_mets[\"FDRD_adfr\"].append(FDRD_adfr)\n",
    "    # fairness_mets[\"FPRD_adfr\"].append(FPRD_adfr)\n",
    "    # fairness_mets[\"FORD_adfr\"].append(FORD_adfr)\n",
    "    # fairness_mets[\"FNRD_adfr\"].append(FNRD_adfr)\n",
    "    fairness_mets[\"adfr\"].append(adfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16ad59-c084-4081-982c-4c5dbeb700c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_df[\"gender\"].value_counts(normalize=False).sort_index().sort_index())\n",
    "display(test_df[\"gender\"].value_counts(normalize=False).sort_index().sort_index())\n",
    "\n",
    "fairness_metrics(\"Random Forest\", clf, X_train_transformed_m, y_train_m, X_train_transformed_f, y_train_f, \"Training\")\n",
    "fairness_metrics(\"Random Forest\", clf, X_test_transformed_m, y_test_m, X_test_transformed_f, y_test_f, \"Testing\")\n",
    "\n",
    "fairness_mets_df = pd.DataFrame(fairness_mets)\n",
    "fairness_mets_df = fairness_mets_df.set_index([\"model\"])\n",
    "fairness_mets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8387e2c-38ed-435a-adc8-6f53ae2ef831",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The model exhibits extreme amounts of bias for male individuals, where `FDRD`, `FPRD`, `FORD`, and `FNRD` have extremely high values that indicate bias in favour of male individuals. Most notably, the `FDRD` metric for the training and testing sets provides values of 9.265201 and 5.298535 respectively, indicating an extremely low precision for predicting high-income male individuals compared to female individuals. The `FNRD` for the training and testing sets provides values of 3.906744 and 2.843385 respectively, indicating an extremely high false negative rate for male individuals compared to female individuals. The value of `adfr` is relatively high, indicating the classifier is relatively unfair. In general, the model is very poor at predicting the true `income` category for male individuals with `income == \">50K\"`.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39261b8e-d798-484a-97ce-9f159f9eb9ed",
   "metadata": {},
   "source": [
    "## Debiasing techniques: dropping protected characteristics\n",
    "\n",
    "A first idea to fix this issue could be dropping the protected characteristics from our dataset before training the classifier. Let's try this out and see if there is any improvement.\n",
    "\n",
    "#### Question 4\n",
    "1. Drop race, gender and native country from training and test set (we are focusing on gender but we will drop race and native country for good measure).\n",
    "2. Transform the cleaned dataset using one-hot encoding.\n",
    "3. Re-train the random forest classifier.\n",
    "4. Compare accuracy and fairness of this new classifier to the previous one. Do we see any improvement? How do you explain the changes you see (or lack thereof)? Note that, to compare fairness, you will need to have a way to identify the gender of each sample, even though you are not using this feature for classification.\n",
    "5. Create a new plot of the feature importance according to this classifier. Do you see any changes from the first one?\n",
    "\n",
    "**Hint:** steps 2, 3 and 5 can be completed by tweaking the starting code given at the beginning of this assignment. Ask a TA or instructor if you need help in doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11c463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer here (you may add more cells)\n",
    "# Step 1 \n",
    "X_train_new = X_train.drop(columns = [\"race\", \"gender\", \"native-country\"])\n",
    "X_test_new = X_test.drop(columns = [\"race\", \"gender\", \"native-country\"])\n",
    "# X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fc733-a31b-4da2-888d-be56d531f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "categorical_feats_new = [\"workclass\",\n",
    "                     \"education\",\n",
    "                     \"marital-status\",\n",
    "                     \"occupation\",\n",
    "                     \"relationship\",\n",
    "                     ]  # Apply one-hot encoding\n",
    "passthrough_feats_new = [\"age\",\n",
    "                \"fnlwgt\",\n",
    "                \"educational-num\",\n",
    "                \"capital-gain\",\n",
    "                \"capital-loss\",\n",
    "                \"hours-per-week\"\n",
    "                ]  # Numerical - no need to scale\n",
    "target = \"income\"\n",
    "\n",
    "ct_new = make_column_transformer(\n",
    "    (\n",
    "        make_pipeline(OneHotEncoder(handle_unknown=\"ignore\",drop=\"if_binary\")),\n",
    "        categorical_feats_new,\n",
    "    ),  # OHE on categorical features\n",
    "    (\"passthrough\", passthrough_feats_new)  # no transformations on numerical features\n",
    ")\n",
    "\n",
    "X_train_transformed_new = ct_new.fit_transform(X_train_new).toarray()\n",
    "\n",
    "column_names_new = list(\n",
    "    ct_new.named_transformers_[\"pipeline\"].get_feature_names_out(\n",
    "        categorical_feats_new\n",
    "    )\n",
    ") + passthrough_feats_new\n",
    "\n",
    "X_test_transformed_new = ct_new.transform(X_test_new).toarray()\n",
    "\n",
    "X_train_transformed_new_df = pd.DataFrame(X_train_transformed_new, columns=column_names_new)\n",
    "X_test_transformed_new_df = pd.DataFrame(X_test_transformed_new, columns=column_names_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6332ad-f51c-43c3-8a97-9e3a4e519ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "clf_new = RandomForestClassifier(random_state=0, max_depth = 19, n_estimators = 100).fit(X_train_transformed_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e520a03-aa46-49c8-86f2-afb8c27a49c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1 \n",
    "print(\"Training accuracy: \" + str(clf.score(X_train_transformed, y_train)))\n",
    "print(\"Testing accuracy: \" + str(clf.score(X_test_transformed, y_test)))\n",
    "print(\"Training accuracy (New): \" + str(clf_new.score(X_train_transformed_new, y_train)))\n",
    "print(\"Testing accuracy (New): \" + str(clf_new.score(X_test_transformed_new, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c732c1-95e4-4842-b9df-e7fbafa674b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2 \n",
    "# Splitting datasets by gender (Despite the variable being removed) \n",
    "\n",
    "train_groups = train_df[\"gender\"]\n",
    "test_groups = test_df[\"gender\"]\n",
    "\n",
    "X_train_transformed_new_m_df = pd.DataFrame(X_train_transformed_new[train_groups == \"Male\"], columns=column_names_new)\n",
    "X_train_transformed_new_f_df = pd.DataFrame(X_train_transformed_new[train_groups == \"Female\"], columns=column_names_new)\n",
    "X_test_transformed_new_m_df = pd.DataFrame(X_test_transformed_new[test_groups == \"Male\"], columns=column_names_new)\n",
    "X_test_transformed_new_f_df = pd.DataFrame(X_test_transformed_new[test_groups == \"Female\"], columns=column_names_new)\n",
    "# display(X_train_transformed_new_m_df.head())\n",
    "\n",
    "fairness_metrics(\"Random Forest\", clf_new, X_train_transformed_new_m_df, y_train_m, X_train_transformed_new_f_df, y_train_f, \"Training (New)\")\n",
    "fairness_metrics(\"Random Forest\", clf_new, X_test_transformed_new_m_df, y_test_m, X_test_transformed_new_f_df, y_test_f, \"Testing (New)\")\n",
    "fairness_mets_df = pd.DataFrame(fairness_mets)\n",
    "fairness_mets_df = fairness_mets_df.set_index([\"model\"])\n",
    "fairness_mets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3d328-5585-4f2a-912d-70ea64f8dd45",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Q4.4:</b>There is relatively minimal changes in accuracy and fairness metrics for each dataset. The accuracy upon the removal of `race`, `gender`, `native-country` increased slightly on the training set from 0.9064318932990143 to 0.9185410512153032, and decreased slightly on the testing set from 0.8624172524397734 to 0.860301644714393. In terms of fairness metrics, there is no change in `PPRD` and `PPGRD`, and the values of `FDRD`, `FPRD`, `FORD`, `FNRD` and `adfr` have increased for the training set and decreased for the testing set.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f26934-f0e4-4369-ad9a-f8c861a4bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 \n",
    "feature_importances_new = clf_new.feature_importances_\n",
    "\n",
    "# Sort the feature importances from greatest to least using the sorted indices\n",
    "sorted_indices_new = feature_importances_new.argsort()[::-1]\n",
    "sorted_feature_names_new = ct_new.get_feature_names_out()[sorted_indices_new]\n",
    "sorted_importances_new = feature_importances_new[sorted_indices_new]\n",
    "\n",
    "# # Create a bar plot of the feature importances\n",
    "sns.set(rc={'figure.figsize':(11.7,30)})\n",
    "sns.barplot(x=sorted_importances_new, y=sorted_feature_names_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6171228c-c453-4949-a267-c2ac0172c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_names_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec224e23-7be7-4bd2-b0c0-6e4cadfbaf92",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Q4.5:</b> The top most important features for the classifier with scores greater than approximately 0.01 are `passthrough__capital-gain`, `pipeline__marital-status_Married-civ-spouse`, `passthrough__age`, `passthrough__educational-num`, `pipeline__relationship_Husband`, `passthrough__fnlwgt`, `passthrough__hours-per-week`, `passthrough__capital-loss`, `pipeline__marital-status_Never-married`, `pipeline__occupation_Exec-managerial`, `pipeline__occupation_Prof-specialty`, `pipeline__education_Bachelors`, `pipeline__relationship_Not-in-family`, `pipeline__relationship_Wife`, `pipeline__education_Masters`, `pipeline__relationship_Own-child`, `pipeline__marital-status_Divorced`, `pipeline__relationship_Unmarried` and `pipeline__education_HS-grad`. The only top most feature that was featured in the original model that did not appear in the above list was `pipeline__gender_Male`, which would have been excluded from the new model from the start.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ffcd0-4c5a-4bc1-840e-c372f54ac529",
   "metadata": {},
   "source": [
    "## Debiasing techniques: undersampling\n",
    "\n",
    "As you should have seen when exploring the dataset, the groups of males and females who make more or less than \\\\$50k are of very different sizes. This alone may have a significant impact on the way the classifier is trained, by teaching it that some groups are much more likely to make more than \\\\$50k than others.\n",
    "\n",
    "Let's try to fix this problem by creating a more balanced training set.\n",
    "\n",
    "#### Question 5\n",
    "1. Run the cell below to create a new training set by selecting a subset of samples from the original one, in which the groups of males and females who make more or less than \\\\$50k are of equal size. To use the maximum number of training samples possible, the size of each group should be equal to the size of the smallest of these groups in the original dataset. **What is the size of each group, and of the final training set?**\n",
    "2. Separate features from target, and transform the cleaned dataset using one-hot encoding. **Remeber to re-transform the test set accordingly!**\n",
    "3. Re-train the random forest classifier.\n",
    "4. Compare accuracy and fairness of this new classifier to the previous ones. Do we see any improvement? How do you explain the changes you see (or lack thereof)? Pay particular attention to the difference in results on the training and test set: can you explain these results?\n",
    "5. Create a new plot of the feature importance according to this classifier. Do you see any changes from the previous ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda565d-fb96-4642-9b79-79d3617e506f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the distribution of gender and income\n",
    "gender_distribution = train_df['gender'].value_counts()\n",
    "income_distribution = train_df['income'].value_counts()\n",
    "\n",
    "# Create balanced subsets\n",
    "balanced_subsets = []\n",
    "smallest = train_df.shape[0]\n",
    "\n",
    "# Finding size of smallest subset by gender and income\n",
    "for gender_category in gender_distribution.index:\n",
    "    for income_category in income_distribution.index:\n",
    "        if train_df[(train_df['gender'] == gender_category) & (train_df['income'] == income_category)].shape[0] < smallest:\n",
    "            smallest = train_df[(train_df['gender'] == gender_category) & (train_df['income'] == income_category)].shape[0]\n",
    "    \n",
    "# Sampling subsets \n",
    "for gender_category in gender_distribution.index:\n",
    "    for income_category in income_distribution.index:\n",
    "        subset = train_df[(train_df['gender'] == gender_category) & (train_df['income'] == income_category)]\n",
    "        subset = subset.sample(smallest)  # Sample to match the minimum count\n",
    "        balanced_subsets.append(subset)\n",
    "        \n",
    "# Merge the balanced subsets to create the final balanced dataset\n",
    "balanced_df = pd.concat(balanced_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2b64b-9313-4ce5-9204-7dc6a90730b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1\n",
    "# Run this cell create training and test sets\n",
    "train_df_bal, test_df_bal = train_test_split(balanced_df, test_size=0.3, random_state=123)\n",
    "\n",
    "X_train_bal, y_train_bal = (\n",
    "    train_df_bal.drop(columns=[\"income\"]),\n",
    "    train_df_bal[\"income\"],\n",
    ")\n",
    "X_test_bal, y_test_bal = (\n",
    "    test_df_bal.drop(columns=[\"income\"]),\n",
    "    test_df_bal[\"income\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e83e04-ce97-40f4-805d-ee1af32eb679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer here (you may add more cells) # Need to refit column transformer due to change in number of samples\n",
    "\n",
    "# STEP 2\n",
    "# Run this cell to do the necessary dataset preprocessing (encoding of categorical features).\n",
    "# Note that, since we are using a tree based classifier, we don't need to scale the \n",
    "# numerical features.\n",
    "categorical_feats = [\"workclass\",\n",
    "                     \"education\",\n",
    "                     \"marital-status\",\n",
    "                     \"occupation\",\n",
    "                     \"relationship\",\n",
    "                     \"race\",\n",
    "                     \"gender\",\n",
    "                     \"native-country\",\n",
    "                     ]  # Apply one-hot encoding\n",
    "passthrough_feats = [\"age\",\n",
    "                \"fnlwgt\",\n",
    "                \"educational-num\",\n",
    "                \"capital-gain\",\n",
    "                \"capital-loss\",\n",
    "                \"hours-per-week\"\n",
    "                ]  # Numerical - no need to scale\n",
    "target = \"income\"\n",
    "\n",
    "ct_bal = make_column_transformer(\n",
    "    (\n",
    "        make_pipeline(OneHotEncoder(handle_unknown=\"ignore\",drop=\"if_binary\")),\n",
    "        categorical_feats,\n",
    "    ),  # OHE on categorical features\n",
    "    (\"passthrough\", passthrough_feats)  # no transformations on numerical features\n",
    ")\n",
    "\n",
    "X_train_transformed_bal = ct_bal.fit_transform(X_train_bal).toarray()\n",
    "\n",
    "column_names_bal = list(\n",
    "    ct_bal.named_transformers_[\"pipeline\"].get_feature_names_out(\n",
    "        categorical_feats\n",
    "    )\n",
    ") + passthrough_feats\n",
    "\n",
    "X_test_transformed_bal = ct_bal.transform(X_test_bal).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d9b1b-c6c1-40f8-8244-707aa628db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_names_bal\n",
    "X_test_transformed_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa09ef-14e2-4277-b27e-24bff697a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3\n",
    "# Run this cell to train a random forest classifer. The hyperparameters have been pre-selected\n",
    "\n",
    "clf_bal = RandomForestClassifier(random_state=0, max_depth = 19, n_estimators = 100).fit(X_train_transformed_bal, y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afbedaf-7f0c-4f68-98bb-ead2d62df3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1 \n",
    "print(\"Training accuracy: \" + str(clf.score(X_train_transformed, y_train)))\n",
    "print(\"Testing accuracy: \" + str(clf.score(X_test_transformed, y_test)))\n",
    "print(\"Training accuracy (Balanced): \" + str(clf_bal.score(X_train_transformed_bal, y_train_bal)))\n",
    "print(\"Testing accuracy (Balanced): \" + str(clf_bal.score(X_test_transformed_bal, y_test_bal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2b4cc-3039-46ae-b734-d6a8106150ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2\n",
    "# Splitting datasets by gender\n",
    "\n",
    "X_train_transformed_bal = pd.DataFrame(X_train_transformed_bal, columns=column_names_bal)\n",
    "X_test_transformed_bal = pd.DataFrame(X_test_transformed_bal, columns=column_names_bal)\n",
    "\n",
    "X_train_transformed_bal_m_df = pd.DataFrame(X_train_transformed_bal[X_train_transformed_bal[\"gender_Male\"] == 1], columns=column_names_bal)\n",
    "X_train_transformed_bal_f_df = pd.DataFrame(X_train_transformed_bal[X_train_transformed_bal[\"gender_Male\"] == 0], columns=column_names_bal)\n",
    "X_test_transformed_bal_m_df = pd.DataFrame(X_test_transformed_bal[X_test_transformed_bal[\"gender_Male\"] == 1], columns=column_names_bal)\n",
    "X_test_transformed_bal_f_df = pd.DataFrame(X_test_transformed_bal[X_test_transformed_bal[\"gender_Male\"] == 0], columns=column_names_bal)\n",
    "\n",
    "y_train_bal_m = train_df_bal[train_df_bal[\"gender\"] == \"Male\"][\"income\"]\n",
    "y_train_bal_f = train_df_bal[train_df_bal[\"gender\"] == \"Female\"][\"income\"]\n",
    "y_test_bal_m = test_df_bal[test_df_bal[\"gender\"] == \"Male\"][\"income\"]\n",
    "y_test_bal_f = test_df_bal[test_df_bal[\"gender\"] == \"Female\"][\"income\"]\n",
    "# display(X_train_transformed_bal_m_df.head())\n",
    "\n",
    "fairness_metrics(\"Random Forest\", clf_bal, X_train_transformed_bal_m_df, y_train_bal_m, X_train_transformed_bal_f_df, y_train_bal_f, \"Training (Balanced)\")\n",
    "fairness_metrics(\"Random Forest\", clf_bal, X_test_transformed_bal_m_df, y_test_bal_m, X_test_transformed_bal_f_df, y_test_bal_f, \"Testing (Balanced)\")\n",
    "fairness_mets_df = pd.DataFrame(fairness_mets)\n",
    "fairness_mets_df = fairness_mets_df.set_index([\"model\"])\n",
    "fairness_mets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf7caf-8903-4939-851c-9b758c298947",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Q5.4: </b>There are some significant changes in the accuracy and fairness metrics for each dataset after undersampling. The accuracy upon undersampling increased slightly on the training set from 0.9064318932990143 to 0.9711181012296254, and decreased slightly on the testing set from 0.8624172524397734 to 0.8432288192128086. There is a substantial decrease in values for most the fairness metrics in the undersampled dataset compared to the original, with the exception of `FDRD` and `FNRD`. `PPRD`, `PPGRD` become very close in value to the reference value of 1, while `adfr` becomes relatively closer in value to the reference value of 0. `FDRD` is decreased for the training set and increased for the testing set, though the distance from the reference value remains relatively similar, while `FNRD` is increased for both the training and testing sets.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa3226-33bb-4757-af29-c10ba45637ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 \n",
    "feature_importances_bal = clf_bal.feature_importances_\n",
    "\n",
    "# Sort the feature importances from greatest to least using the sorted indices\n",
    "sorted_indices_bal = feature_importances_bal.argsort()[::-1]\n",
    "sorted_feature_names_bal = ct_bal.get_feature_names_out()[sorted_indices_bal]\n",
    "sorted_importances_bal = feature_importances_bal[sorted_indices_bal]\n",
    "\n",
    "# # Create a bar plot of the feature importances\n",
    "sns.set(rc={'figure.figsize':(11.7,30)})\n",
    "sns.barplot(x=sorted_importances_bal, y=sorted_feature_names_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a13b6-1a63-4de1-99fe-59810ab03665",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_names_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59f336-171a-48f8-8e3d-81a5c17938de",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Q5.5:</b>The top most important features for the classifier with scores greater than approximately 0.01 are `passthrough__age`, `pipeline__marital-status_Married-civ-spouse`, `passthrough__educational-num`, `passthrough__capital-gain`, `passthrough__fnlwgt`, `passthrough__hours-per-week`, `pipeline__relationship_Wife`, `pipeline__marital-status_Never-married`, `pipeline__occupation_Exec-managerial`, `pipeline__relationship_Husband`, `passthrough__capital-loss`, `pipeline__relationship_Own-child`, `pipeline__occupation_Prof-specialty`, `pipeline__education_Bachelors`, `pipeline__gender_Male`, `pipeline__relationship_Not-in-family`, `pipeline__education_Masters`, `pipeline__workclass_Private`, `pipeline__marital-status_Divorced`, `pipeline__occupation_Other-service`, `pipeline__relationship_Unmarried`, and `pipeline__education_HS-grad`. Compared to the original model, the features `pipeline__workclass_Private`, `pipeline__occupation_Other-service`, `pipeline__relationship_Unmarried` and `pipeline__education_HS-grad` have significantly greater importance relative to the other model features.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c910ae2-813b-4fa6-9df9-6e5ce2c1c53e",
   "metadata": {},
   "source": [
    "## Debiasing techniques: oversampling (with SMOTE)\n",
    "\n",
    "Another way to create a more balanced training set, but without sacrificing training samples, is by *oversampling*, which means artificially increasing the size of the training set with \"fake\" samples. This can be achieved mainly in two ways:\n",
    "1. By resampling (replicating) samples from the original training set, or\n",
    "2. By introducing artificial *new* samples, similar enough to those included in the original training set\n",
    "\n",
    "The Synthetic Minority Oversampling Technique (SMOTE) seen in class falls in the second group. In this portion of the assignment, you will create a more balanced dataset using SMOTE (specifically [SMOTENC](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html), a version of SMOTE that allows working with categorical variables).\n",
    "\n",
    "#### Question 6\n",
    "1. Run the cell below to create a more balanced training set using SMOTE. Note that a large portion of code is replicated to guarantee that the correct data is used, and not one modified in previous cells. The actual rebalancing all happens in the last 2 lines.\n",
    "2. Explore the new training set, and provide the following information: what is the size of the new training set? Is the target variable balanced? How many samples are classified as >\\\\$50, and how many as <=\\\\$50k? Is the target variable balanced across protected groups, or at least more balanced than before? How many males and females are classified as >\\\\$50, and how many as <=\\\\$50k?\n",
    "3. Re-train the random forest classifier.\n",
    "4. Compare accuracy and fairness of this new classifier to the previous ones. Do we see any improvement? How do you explain the changes you see (or lack thereof)? Pay particular attention to the difference in results on the training and test set: can you explain these results?\n",
    "5. Create a new plot of the feature importance according to this classifier. Do you see any changes from the previous ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "X_train, y_train = (\n",
    "    train_df.drop(columns=[\"income\"]),\n",
    "    train_df[\"income\"],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    test_df.drop(columns=[\"income\"]),\n",
    "    test_df[\"income\"],\n",
    ")\n",
    "\n",
    "oversample = SMOTENC(categorical_features=categorical_feats, random_state=0)\n",
    "\n",
    "X_train_SMOTE, y_train_SMOTE = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277dd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation applied after oversampling\n",
    "\n",
    "categorical_feats = [\"workclass\",\n",
    "                     \"education\",\n",
    "                     \"marital-status\",\n",
    "                     \"occupation\",\n",
    "                     \"relationship\",\n",
    "                     \"race\",\n",
    "                     \"gender\",\n",
    "                     \"native-country\",\n",
    "                     ]  # Apply one-hot encoding\n",
    "passthrough_feats = [\"age\",\n",
    "                \"fnlwgt\",\n",
    "                \"educational-num\",\n",
    "                \"capital-gain\",\n",
    "                \"capital-loss\",\n",
    "                \"hours-per-week\"\n",
    "                ]  # Numerical - no need to scale\n",
    "target = \"income\"\n",
    "\n",
    "\n",
    "ctSMOTE = make_column_transformer(\n",
    "    (\n",
    "        OneHotEncoder(handle_unknown=\"ignore\",drop=\"if_binary\",sparse_output=False),\n",
    "        categorical_feats,\n",
    "    ),  # OHE on categorical features\n",
    "    (\"passthrough\", passthrough_feats)  # no transformations on numerical features\n",
    ")\n",
    "\n",
    "# X_train_transformed = ctSMOTE.fit_transform(X_train_SMOTE)\n",
    "# X_test_transformed = ctSMOTE.transform(X_test)\n",
    "X_train_transformed_SMOTE = ctSMOTE.fit_transform(X_train_SMOTE)\n",
    "X_test_transformed_SMOTE = ctSMOTE.transform(X_test)\n",
    "\n",
    "# Column names, if needed\n",
    "column_names_SMOTE = list(\n",
    "    ctSMOTE.named_transformers_[\"onehotencoder\"].get_feature_names_out(\n",
    "        categorical_feats\n",
    "    )\n",
    ") + passthrough_feats\n",
    "\n",
    "# X_train_transformed and X_test_transformed can now be used to answer the questions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (you may add more cells)\n",
    "train_df_SMOTE = pd.concat([X_train_SMOTE, y_train_SMOTE], axis=1)\n",
    "\n",
    "print(train_df_SMOTE.size) # 779520\n",
    "print(train_df_SMOTE.shape) # (51968, 15)\n",
    "display(y_train_SMOTE.value_counts(normalize=True).sort_index()) # Balance?\n",
    "display(y_train_SMOTE.value_counts(normalize=False).sort_index()) # Number of each \n",
    "# display(train_df_SMOTE.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d012ee-d8f8-4613-a052-a6fd5f57b4c9",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Q6.2.1, 6.2.2, 6.2.3:</b>The size and of the new training set are 779520 and (51968, 15) respectively. Overall, the target variable is balanced, with 25984 samples classified as `income == \"<=50K\"` and 25984 samples classified as `income == \">50K\"`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3070c68-874f-4ea3-8960-695e755351b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = df[\"gender\"].astype(\"string\").value_counts().sort_index().index\n",
    "genders_2 = train_df_SMOTE[\"gender\"].astype(\"string\").value_counts().sort_index().index\n",
    "# print(genders)\n",
    "# print(genders_2)\n",
    "\n",
    "for gen in genders_2: \n",
    "    data = df[df[\"gender\"] == gen]\n",
    "    print(\"Distribution of income for \" + gen.lower() + \" overall\")\n",
    "    display(data[\"income\"].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    data_2 = train_df_SMOTE[train_df_SMOTE[\"gender\"] == gen]\n",
    "    print(\"Distribution of income for \" + gen.lower() + \" in SMOTE training set\")\n",
    "    display(data_2[\"income\"].value_counts(normalize=True).sort_index())\n",
    "\n",
    "for gen in genders_2: \n",
    "    data_2 = train_df_SMOTE[train_df_SMOTE[\"gender\"] == gen]\n",
    "    print(\"Count of income for \" + gen.lower() + \" in SMOTE training set\")\n",
    "    display(data_2[\"income\"].value_counts(normalize=False).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d82f45-eaa3-43cb-97da-3a41b5071210",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Q6.2.4.1, 6.2.5:</b>The target variable `income` is only slightly more balanced across each `gender` group compared to the original dataset, and the distributions of each `income` category are not equal across each level of `gender`. For instance, the proportion of `income == \"<=50K\"` samples is 0.836503 among female individuals and 0.398497 among male individuals. There are 15910 males and 10074 females classified as `income == \"<=50K\"`, and 24015 males and 1969 females classified as `income == \"<=50K\"`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e055901-21a6-4ffe-81b4-c769384c267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df[\"race\"].astype(\"string\").value_counts().sort_index().index\n",
    "races_2 = train_df_SMOTE[\"race\"].astype(\"string\").value_counts().sort_index().index\n",
    "# print(races)\n",
    "# print(races_2)\n",
    "\n",
    "for gen in races_2: \n",
    "    data = df[df[\"race\"] == gen]\n",
    "    print(\"Distribution of income for \" + gen.lower() + \" overall\")\n",
    "    display(data[\"income\"].value_counts(normalize=True).sort_index())\n",
    "\n",
    "    data_2 = train_df_SMOTE[train_df_SMOTE[\"race\"] == gen]\n",
    "    print(\"Distribution of income for \" + gen.lower() + \" in SMOTE training set\")\n",
    "    display(data_2[\"income\"].value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6037b-4cb7-4823-a7da-ab942aa85200",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Q6.2.4.2:</b>The target variable `income` is only slightly more balanced across each `gender` group \n",
    "Compared to the original dataset, the target variable `income` is significantly more balanced for `race == \"white\"`, only slightly more balanced for `race == \"asian-pac-islander\"` and `race == \"black\"`, and slightly less balanced for `race == \"amer-indian-eskimo\"` and `race == \"other\"`. The distributions of each `income` category are not equal across each level of `gender`. For instance, the proportion of `income == \"<=50K\"` samples is 0.904908 among amer-indian-eskimo individuals and 0.464996 among white individuals.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd67996-5a9c-4511-81b2-841d61806585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "clf_SMOTE = RandomForestClassifier(random_state=0, max_depth = 19, n_estimators = 100).fit(X_train_transformed_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d796d-aef8-45b4-96c0-5e8433623826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1 \n",
    "print(\"Training accuracy: \" + str(clf.score(X_train_transformed, y_train)))\n",
    "print(\"Testing accuracy: \" + str(clf.score(X_test_transformed, y_test)))\n",
    "print(\"Training accuracy (SMOTE): \" + str(clf_SMOTE.score(X_train_transformed_SMOTE, y_train_SMOTE)))\n",
    "print(\"Testing accuracy (SMOTE): \" + str(clf_SMOTE.score(X_test_transformed_SMOTE, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e089b-85c9-4b6f-a37f-37416d003312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2\n",
    "# Splitting datasets by gender\n",
    "\n",
    "X_train_transformed_SMOTE = pd.DataFrame(X_train_transformed_SMOTE, columns=column_names_SMOTE)\n",
    "X_test_transformed_SMOTE = pd.DataFrame(X_test_transformed_SMOTE, columns=column_names_SMOTE)\n",
    "\n",
    "X_train_transformed_SMOTE_m_df = pd.DataFrame(X_train_transformed_SMOTE[X_train_transformed_SMOTE[\"gender_Male\"] == 1], columns=column_names_SMOTE)\n",
    "X_train_transformed_SMOTE_f_df = pd.DataFrame(X_train_transformed_SMOTE[X_train_transformed_SMOTE[\"gender_Male\"] == 0], columns=column_names_SMOTE)\n",
    "X_test_transformed_SMOTE_m_df = pd.DataFrame(X_test_transformed_SMOTE[X_test_transformed_SMOTE[\"gender_Male\"] == 1], columns=column_names_SMOTE)\n",
    "X_test_transformed_SMOTE_f_df = pd.DataFrame(X_test_transformed_SMOTE[X_test_transformed_SMOTE[\"gender_Male\"] == 0], columns=column_names_SMOTE)\n",
    "\n",
    "y_train_SMOTE_m = train_df_SMOTE[train_df_SMOTE[\"gender\"] == \"Male\"][\"income\"]\n",
    "y_train_SMOTE_f = train_df_SMOTE[train_df_SMOTE[\"gender\"] == \"Female\"][\"income\"]\n",
    "y_test_SMOTE_m = test_df[test_df[\"gender\"] == \"Male\"][\"income\"]\n",
    "y_test_SMOTE_f = test_df[test_df[\"gender\"] == \"Female\"][\"income\"]\n",
    "# display(X_train_transformed_bal_m_df.head())\n",
    "\n",
    "fairness_metrics(\"Random Forest\", clf_SMOTE, X_train_transformed_SMOTE_m_df, y_train_SMOTE_m, X_train_transformed_SMOTE_f_df, y_train_SMOTE_f, \"Training (SMOTE)\")\n",
    "fairness_metrics(\"Random Forest\", clf_SMOTE, X_test_transformed_SMOTE_m_df, y_test_SMOTE_m, X_test_transformed_SMOTE_f_df, y_test_SMOTE_f, \"Testing (SMOTE)\")\n",
    "fairness_mets_df = pd.DataFrame(fairness_mets)\n",
    "fairness_mets_df = fairness_mets_df.set_index([\"model\"])\n",
    "fairness_mets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f75743-a2b6-4cd3-a686-ac1f7fecdbfa",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Q6.4:</b> There are some significant changes in the accuracy and fairness metrics for each dataset after using SMOTE. The accuracy upon undersampling increased slightly on the training set from 0.9064318932990143 to 0.9203163485221675, and decreased slightly on the testing set from 0.8624172524397734 to 0.8413294205964649. There is a substantial increase in values for most the fairness metrics in the undersampled dataset compared to the original, with the exception of `FDRD` and `FPRD`. `PPRD`, and`PPGRD` are increased significantly for the training set and remain the same for the testing set, while `FORD` is increased significantly for both the training and testing sets. `FDRD` and `FPRD` are decreased significantly for both the training and testing sets, with `FDRD` becoming relatively further in value from the reference value of 1, while `FNRD` is decreased significantly for the training set and increased significantly for the testing set. As a consequence, `adfr` is increased significantly for both the training and testing sets.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1458e-6cb5-4208-ae2f-35b437477407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 \n",
    "feature_importances_SMOTE = clf_SMOTE.feature_importances_\n",
    "\n",
    "# Sort the feature importances from greatest to least using the sorted indices\n",
    "sorted_indices_SMOTE = feature_importances_SMOTE.argsort()[::-1]\n",
    "sorted_feature_names_SMOTE = ctSMOTE.get_feature_names_out()[sorted_indices_SMOTE]\n",
    "sorted_importances_SMOTE = feature_importances_SMOTE[sorted_indices_SMOTE]\n",
    "\n",
    "# # Create a bar plot of the feature importances\n",
    "sns.set(rc={'figure.figsize':(11.7,30)})\n",
    "sns.barplot(x=sorted_importances_SMOTE, y=sorted_feature_names_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670fdd5c-72f6-460d-91b4-9a0c44339012",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_names_SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23488d7-6914-4548-a195-200a4832936d",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Q6.5:</b> The top most important features for the classifier with scores greater than approximately 0.01 are `onehotencoder__marital-status_Married-civ-spouse`, `onehotencoder__relationship_Husband`, `passthrough__age`, `onehotencoder__marital-status_Never-married`, `passthrough__educational-num`, `passthrough__capital-gain`, `passthrough__hours-per-week`, `onehotencoder__occupation_Exec-managerial`, `onehotencoder__gender_Male`, `onehotencoder__relationship_Not-in-family`, `onehotencoder__education_Bachelors`, `passthrough__fnlwgt`, `onehotencoder__occupation_Prof-specialty`, `onehotencoder__relationship_Own-child`, `onehotencoder__marital-status_Divorced`, `passthrough__capital-loss`, `onehotencoder__workclass_Private`, `onehotencoder__native-country_United-States`, `onehotencoder__occupation_Other-service`, `onehotencoder__relationship_Unmarried`, and `onehotencoder__race_White`. Compared to the previous models, the features `onehotencoder__native-country_United-States`and `onehotencoder__race_White` have significantly greater importance relative to the other model features. `pipeline__education_Masters` has significantly less importance compared to the original model, while `pipeline__education_HS-grad` has significantly less importance compared to the undersampling model.\n",
    "</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9846f4d",
   "metadata": {},
   "source": [
    "## Equalized odd post processing\n",
    "\n",
    "An alternative to the methods seen so far (which may produce unsatisfactory results), is applying post-processing to the predictions of the classifier, so that they optimize equalized odds (whether the TPR and FPR are on par across groups).\n",
    "\n",
    "`aif360`, a popular open-source library dedicated to detecting and mitigating bias in machine learning models, includes [`EqOddsPostprocessing`](https://aif360.readthedocs.io/en/stable/modules/generated/aif360.algorithms.postprocessing.EqOddsPostprocessing.html), a function to performe equalized odds post-processing. The function is slightly more intricate to use than others you have used so far (typically from `sklearn`), so we will see together how to apply it on the test (you may try and replicate this on the training set for your own practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f562a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to reset training and test sets (and clear accidental prior changes)\n",
    "\n",
    "X_train, y_train = (\n",
    "    train_df.drop(columns=[\"income\"]),\n",
    "    train_df[\"income\"],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    test_df.drop(columns=[\"income\"]),\n",
    "    test_df[\"income\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210651b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to do the necessary dataset preprocessing (encoding of categorical features).\n",
    "# Note that, since we are using a tree based classifier, we don't need to scale the \n",
    "# numerical features.\n",
    "\n",
    "categorical_feats = [\"workclass\",\n",
    "                     \"education\",\n",
    "                     \"marital-status\",\n",
    "                     \"occupation\",\n",
    "                     \"relationship\",\n",
    "                     \"race\",\n",
    "                     \"gender\",\n",
    "                     \"native-country\",\n",
    "                     ]  # Apply one-hot encoding\n",
    "passthrough_feats = [\"age\",\n",
    "                \"fnlwgt\",\n",
    "                \"educational-num\",\n",
    "                \"capital-gain\",\n",
    "                \"capital-loss\",\n",
    "                \"hours-per-week\"\n",
    "                ]  # Numerical - no need to scale\n",
    "target = \"income\"\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (\n",
    "        make_pipeline(OneHotEncoder(handle_unknown=\"ignore\",drop=\"if_binary\")),\n",
    "        categorical_feats,\n",
    "    ),  # OHE on categorical features\n",
    "    (\"passthrough\", passthrough_feats)  # no transformations on numerical features\n",
    ")\n",
    "\n",
    "X_train_transformed = ct.fit_transform(X_train).toarray()\n",
    "X_test_transformed = ct.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a8815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to pandas dataframes\n",
    "\n",
    "column_names = list(\n",
    "    ct.named_transformers_[\"pipeline\"].get_feature_names_out(\n",
    "        categorical_feats\n",
    "    )\n",
    ") + passthrough_feats\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train_transformed, columns=column_names)\n",
    "X_test_df = pd.DataFrame(X_test_transformed, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1894ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0, max_depth = 19, n_estimators = 100).fit(X_train_df, y_train)\n",
    "\n",
    "# Get predictions for test set \n",
    "y_pred = clf.predict(X_test_df)\n",
    "\n",
    "# So far, all this is the same as the biased classifier we started with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data into a BinaryLabelDataset, necessary to work in aif360\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "X_test_df = X_test_df.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "y_binary = y_test.map({'>50K': 1, '<=50K': 0})  # Map categorical values to binary\n",
    "\n",
    "test_mld = BinaryLabelDataset(df=pd.concat([X_test_df, y_binary], axis=1),\n",
    "                              label_names=['income'],\n",
    "                              protected_attribute_names=['gender_Male'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataset with predicted labels for comparison\n",
    "test_pred_mld = test_mld.copy()\n",
    "\n",
    "# Convert to binary label (e.g., class 2 is positive, others are negative)\n",
    "y_pred_binary = np.where(y_pred == '>50K', 1, 0)\n",
    "\n",
    "test_pred_mld.labels = y_pred_binary.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063be1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "\n",
    "# Initialize EqOddsPostprocessing\n",
    "eq_odds = EqOddsPostprocessing(unprivileged_groups=[{'gender_Male': 0}],\n",
    "                               privileged_groups=[{'gender_Male': 1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d62114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the EqOddsPostprocessing model # Changing of predictions \n",
    "eq_odds = eq_odds.fit(test_mld, test_pred_mld)\n",
    "\n",
    "# Get new fair predictions\n",
    "fair_pred_mld = eq_odds.predict(test_pred_mld)\n",
    "\n",
    "# Convert predictions back to array\n",
    "fair_predictions = fair_pred_mld.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified predictions\n",
    "fair_predictions_cat = np.where(fair_predictions == 1, '>50K', '<=50K')\n",
    "fair_predictions_cat "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b83514",
   "metadata": {},
   "source": [
    "`fair_predictions_cat` now includes the post-processed predictions, after equalized odds postprocessing. \n",
    "\n",
    "#### Question 7\n",
    "\n",
    "Compute accuracy and fairness of this new predictions, and compare the results to the previous ones. Do we see any improvement? Is this technique more or less effective than the others tried before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccc9f7-2220-41c6-9417-cdf7f09552d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: \" + str(clf.score(X_train_transformed, y_train)))\n",
    "print(\"Testing accuracy: \" + str(clf.score(X_test_transformed, y_test)))\n",
    "# print(\"Training accuracy (EqOddsPostprocessing): \" + str(clf.score(X_train_df, fair_predictions_cat))) # NOT SURE WHAT TO DO WITH THIS\n",
    "print(\"Testing accuracy (EqOddsPostprocessing): \" + str(clf.score(X_test_df, fair_predictions_cat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6429d-3922-438a-b161-471b050ed3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_eq_odds = pd.DataFrame(fair_predictions_cat, columns = [\"income\"])\n",
    "test_df_eq_odds = pd.concat([X_test_df, y_test_eq_odds], axis=1)\n",
    "# test_df_eq_odds[\"gender_Male\"]\n",
    "# display(test_df_eq_odds.head())\n",
    "\n",
    "X_test_eq_odds_m = pd.DataFrame(X_test_df[X_test_df[\"gender_Male\"] == 1])\n",
    "X_test_eq_odds_f = pd.DataFrame(X_test_df[X_test_df[\"gender_Male\"] == 0])\n",
    "\n",
    "y_test_eq_odds_m = test_df_eq_odds[test_df_eq_odds[\"gender_Male\"] == 1][\"income\"]\n",
    "y_test_eq_odds_f = test_df_eq_odds[test_df_eq_odds[\"gender_Male\"] == 0][\"income\"]\n",
    "# display(X_test_eq_odds_m.head())\n",
    "\n",
    "fairness_metrics(\"Random Forest\", clf, X_test_eq_odds_m, y_test_eq_odds_m, X_test_eq_odds_f, y_test_eq_odds_f, \"Testing (EqOddsPostprocessing)\")\n",
    "fairness_mets_df = pd.DataFrame(fairness_mets)\n",
    "fairness_mets_df = fairness_mets_df.set_index([\"model\"])\n",
    "fairness_mets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade430f-3be0-40bb-8b75-24815e3f0bc4",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">TEXT</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3ff92-6128-4a7d-afca-4ff69b28588b",
   "metadata": {},
   "source": [
    "## Final remarks\n",
    "\n",
    "#### Question 8\n",
    "\n",
    "Based on the results seen so far, provide an overall evaluation of our debiasing efforts. In particular, try answering the following questions:\n",
    "1. What do you think was the most successful technique? Which one was the least successful? \n",
    "2. If you found that bias still persists after attempting a debiasing strategy, what do you think is the reason? What could be done to fix this problem?\n",
    "\n",
    "(max 400 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b3b8a-bdc5-4f12-ab4b-f3999a1f07e4",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">TEXT</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90f251-74e8-44da-b2b0-253307e63136",
   "metadata": {},
   "source": [
    "# Final thoughts\n",
    "\n",
    "1) If you have completed this assignment in a group, please write a detailed description of how you divided the work and how you helped each other completing it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455b783-b858-4251-9a3c-b3829fc1a6c9",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">We worked on the assignment separately, each taking turns to answer all parts and modifying the responses down the line.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659a333-01be-4c52-a54c-5ee16726ff9f",
   "metadata": {},
   "source": [
    "2) Have you used ChatGPT or a similar Large Language Model (LLM) to complete this homework? Please describe how you used the tool. We will never deduct points for using LLMs for completing homework assignments, but this helps us understand how you are using the tool and advise you in case we believe you are using it incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ab1fd-5925-4fc5-811b-da56d06247e7",
   "metadata": {},
   "source": [
    "* <span style=\"color:blue\">Jingyuan's response: </span>\n",
    "* <span style=\"color:blue\">Nicholas' response: </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c40e0-bdca-45d1-87f9-901ae05db256",
   "metadata": {},
   "source": [
    "3) Have you struggled with some parts (or all) of this homework? Do you have pending questions you would like to ask? Write them down here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bb471-0278-45d3-8881-2810381e62b4",
   "metadata": {},
   "source": [
    "* <span style=\"color:blue\">Jingyuan's response: </span>\n",
    "* <span style=\"color:blue\">Nicholas' response: How to interpret the usage of equalized odd post processing for accuracy and fairness metrics.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4f434-7134-4af5-812f-417d9354410a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSCI 430",
   "language": "python",
   "name": "dsci430"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
