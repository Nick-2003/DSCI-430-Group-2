{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ee47c6",
   "metadata": {},
   "source": [
    "# Module 5 - Bias in Word Embeddings\n",
    "\n",
    "### Assignment overview\n",
    "\n",
    "In this assignment, you will be asked to evaluate gender bias in word embeddings, debias the embeddings in post-processing, and determine the extent to which the bias is still present after debiasing.\n",
    "\n",
    "The assignment is modeled after “Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them, by Hila Gonen and Yoav Goldberg (https://arxiv.org/pdf/1903.03862.pdf). \n",
    "\n",
    "For this assignment, it is possible to work in **groups of up to 2 students**. \n",
    "\n",
    "### Group members\n",
    "Leave blanks if group has less than 2 members:\n",
    "- Student 1: Jingyuan Liu (S.N. 69763183)\n",
    "- Student 2: Nicholas Tam (S.N. 45695970)\n",
    "\n",
    "\n",
    "### Learning Goals:\n",
    "\n",
    "After completing this assignment, you will be able to:\n",
    "1. Evaluate the extent of language bias in word embeddings (pre-trained or trained on a new corpus of text) \n",
    "2. Apply a post-processing approach to reduce the presence of stereotypes in word embeddings; recognize and explain their limitations \n",
    "3. Evaluate the presence of bias in word embeddings after the application of de-biasing strategies \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1c3cc",
   "metadata": {},
   "source": [
    "## 1. Introduction to NLP and Word Embeddings \n",
    "\n",
    "Natural language processing, or NLP for short, is the study of how to use computers in order to process, analyze, and produce language similar to how a human would. You've come across NLP if you've used:\n",
    "\n",
    "- Google Translate\n",
    "- Speech to text tools\n",
    "- And of course, ChatGPT...\n",
    "\n",
    "The question is: how do we go from words in a language to something a computer can understand? One of the answers is word embeddings. Word embeddings are a way to represent a word using a vector of numbers, in a way such that similar words will have vectors that are closer to each other. For example, when we convert the word \"bad\" into a word embedding, we would expect it to be close to the embedding for \"worst\", but maybe not as close to the word \"rainbow\".\n",
    "\n",
    "<center>\n",
    "<img src=\"wordEmbeddings.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "There are many different pre-trained word embeddings out there, trained on different kinds of data. In this module we will use pre-trained embeddings (no need to create our own). Before we do that, let's get some more familiarity with vectors and their operations..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e94a9",
   "metadata": {},
   "source": [
    "### Vectors and Vector Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0025b82d",
   "metadata": {},
   "source": [
    "To better understand word embeddings, we first need to know how these words are represented - vectors - and what operations can be applied to them. A vector is a type of data that has a magnitude and direction. Vectors can exist in spaces of any dimension. For example, $\\vec{[1, 2]}$ is a vector in 2D space, and $\\vec{[4, 1, 7]}$ is a vector in 3D space. Let's work in 2D space, to make it easier to visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75fb41",
   "metadata": {},
   "source": [
    "Suppose we have two vectors:\n",
    "    \n",
    "$a = \\vec{[1, 2]}$\n",
    "\n",
    "$b = \\vec{[-3, 1]}$\n",
    "\n",
    "Let's start by plotting $a$ in red and $b$ in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95898bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.array([1, 2]) # vector a\n",
    "b = np.array([-3, 1]) # vector b\n",
    "origin = np.array([[0, 0],[0, 0]]) # origin point\n",
    "\n",
    "# plotting\n",
    "V = np.array([a, b])\n",
    "plt.quiver(*origin, V[:,0], V[:,1], color=['r','b'], angles='xy', scale_units='xy', scale=1, width=0.015)\n",
    "plt.text(V[0,0] + 0.2, V[0,1], 'a', fontsize=14, color='red')\n",
    "plt.text(V[1,0] - 0.4, V[1,1] + 0.2, 'b', fontsize=14, color='blue')\n",
    "plt.xlim((-5,5))\n",
    "plt.ylim((-5,5))\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487c08a",
   "metadata": {},
   "source": [
    "Now, we will try adding $a$ and $b$. To do this, all we have to do is sum the first coordinate of $a$ with the first coordinate of $b$, and the second coordinate of $a$ with the second coordinate of $b$. \n",
    "\n",
    "We get $1 + (-3) = -2$ for the first coordinate, and $2 + 1 = 3$ for the second coordinate. So, $a + b = \\vec{[-2, 3]}$.\n",
    "\n",
    "\n",
    "But this has more than just an abstract meaning. It makes sense when you think about it in terms of geometry too. In geometrical terms, going in the direction of $a + b$ means going in the direction of $a$ for $a$'s magnitude, then the direction of $b$ for $b$'s magnitude. Let's see what it looks like, in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1f0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "origin = np.array([[0, 1, 0],[0, 2, 0]]) # origin point; for b, the origin is now the\n",
    "                                         # tip of a (1,2)\n",
    "\n",
    "V = np.array([a, b, a + b])\n",
    "plt.quiver(*origin, V[:,0], V[:,1], color=['r','b','g'], angles='xy', scale_units='xy', scale=1, width=0.015)\n",
    "plt.text(V[0,0] + 0.2, V[0,1], 'a', fontsize=14, color='red')\n",
    "plt.text(V[1,0] + 1, V[1,1] + 2.4, 'b', fontsize=14, color='blue')\n",
    "plt.text(V[2,0] - 1.5, V[2,1] - 2, 'a + b', fontsize=14, color='green')\n",
    "plt.xlim((-5,5))\n",
    "plt.ylim((-5,5))\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1305e4",
   "metadata": {},
   "source": [
    "Now that we have a taste for vector addition, let's try some vector subtraction. To find $a - b$, we subtract the first coordinate of $b$ from the first coordinate of $a$, and the second coordinate of $b$ from the second coordinate of $a$. \n",
    "\n",
    "We get $1 - (-3) = 4$ for the first coordinate, and $2 - 1 = 1$ for the second coordinate. So $a - b = \\vec{[4, 1]}$. \n",
    "\n",
    "In geometrical terms, going in the direction of $a - b$ means going in the direction of $a$ for $a$'s magnitude, then the OPPOSITE direction of $b$ for $b$'s magnitude. Let's see what it looks like, in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71f450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "origin = np.array([[0, 1, 0],[0, 2, 0]]) # origin point; again, for b, the origin is now the\n",
    "                                         # tip of a (1,2)\n",
    "\n",
    "V = np.array([a, -b, a - b]) # -b means we inverted its direction\n",
    "plt.quiver(*origin, V[:,0], V[:,1], color=['r','b', 'g'], angles='xy', scale_units='xy', scale=1, width=0.015)\n",
    "plt.text(V[0,0], V[0,1] + 0.5, 'a', fontsize=14, color='red')\n",
    "plt.text(V[1,0] + 1, V[1,1] + 2.4, 'b', fontsize=14, color='blue')\n",
    "plt.text(V[2,0]- 2.5, V[2,1] - 1.5, 'a - b', fontsize=14, color='green')\n",
    "plt.xlim((-5,5))\n",
    "plt.ylim((-5,5))\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58905ab",
   "metadata": {},
   "source": [
    "Finally, we'll talk about the dot product. The dot product of two vectors is a measure for how similar of a direction they are pointing in. The dot product of $a$ with $b$ shows us the amount that $a$ is pointing in the same direction as $b$. To calculate the dot product, multiply each coordinate of $a$ with its corresponding coordinate of $b$, then add everything together. \n",
    "\n",
    "$a \\cdot b = 1(-3) + 2(1) = -1$\n",
    "\n",
    "In geometrical terms, if the dot product is a **positive number**, the vectors form an acute angle with one another (they are pointing in **similar directions**). If the dot product is a **negative number**, the vectors form an obtuse angle with one another (they are pointing in **opposite directions**). And if the dot product is 0, the vectors form a right angle with one another (they are pointing in perpendicular directions). So it makes sense that the dot product of $a$ and $b$ is negative because the angle between them is obtuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a5a88",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Your turn! Complete the following operations on these vectors, which now have three dimensions.\n",
    "\n",
    "$a = \\vec{[3, 1, 2]}$ \n",
    "\n",
    "$b = \\vec{[-1, 2, 1]}$\n",
    "\n",
    "- What is $a + b$?\n",
    "- What is $a - b$?\n",
    "- What is $a \\cdot b$? Are they pointing in the same or in the opposite direction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5c314",
   "metadata": {},
   "source": [
    "* <span style=\"color:blue\">$a + b = \\vec{[2, 3, 3]}$</span>\n",
    "* <span style=\"color:blue\">$a - b = \\vec{[4, -1, 1]}$</span>\n",
    "* <span style=\"color:blue\">$a \\cdot b = \\vec{[-3, 2, 2]}$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd7422",
   "metadata": {},
   "source": [
    "### Load GloVe Embeddings\n",
    "\n",
    "We are now ready to start working with some word embeddings. As mentioned before, we will work with pre-trained model. For this portion of the exercise, we will work with GloVe, pre-trained on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49e026-c35a-4b5a-adad-20471bb7d4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You will need to run this cell once to install the new package. After that, comment out the command\n",
    "# !pip install gensim\n",
    "# conda install gensim -n DSCI430 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74212d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fe310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This may take a couple minutes to load.\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Disclaimer: glove-wiki-gigaword-100 is a pre-trained model of 400000 records, trained on Wikipedia. It is\n",
    "# much smaller than models used for real applications, and as such it will not always behave one\n",
    "# would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c17884",
   "metadata": {},
   "source": [
    "### Similarities and Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e22de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A list of words - you can try others if you'd like!\n",
    "lst = ['librarian', 'cash', 'president', 'exercise', 'dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba0eef-adc9-4104-ba12-57cd8dcd324e",
   "metadata": {},
   "source": [
    "Look how easy it is to find similar words using word embeddings - all we have to do is look for the next nearest vectors! And the GloVe API has a function to do this - let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc7f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word in lst: \n",
    "    similar_words = model.most_similar(word) # getting the most similar word using GloVe API\n",
    "                                             # by default, it returns the 10 closest words\n",
    "    similar_words = [w[0] for w in similar_words]\n",
    "    print(f\"Similar words for '{word}': {', '.join(similar_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f661f4",
   "metadata": {},
   "source": [
    "Another great property of word embeddings is that analogies between words is represented as a simple vector operation. Look at these examples ([source](https://towardsdatascience.com/a-guide-to-word-embeddings-8a23817ab60f)):\n",
    "\n",
    "<center>\n",
    "<img src=\"vectorSimilarities.png\" width=\"900\"/>\n",
    "</center>\n",
    "\n",
    "Look at the plot on the left: the difference \"man/woman\" can be thought of as the distance between two vectors, and it should be the same for each pair of gendered words (man/woman, king/queen, actor/actress...). If I can compute this distance from a pair (for example, from the man/woman pair), I can apply it to another word (\"king\") and get to the other word in the pair (\"queen\"). This is true for any relationship between words (in the picture, gerund and past tense of a verb, and country and capital)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c25fff5-02e3-4094-9794-bf6fd4277415",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Given the vectors for the words $\\vec{KING}$, $\\vec{MAN}$ and $\\vec{WOMAN}$, what operation (addition, subtraction, or a combination of) will lead me to the word $\\vec{QUEEN}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4fee4f-e580-4b48-8fcf-3fc1b0483d06",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">$\\vec{KING} + (\\vec{WOMAN} - \\vec{MAN}) = \\vec{QUEEN}$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678c0f4",
   "metadata": {},
   "source": [
    "We will now play around with analogies. Analogies are patterns of the form:\n",
    "\n",
    "- Japan is to as Tokyo as India is to Delhi\n",
    "- hand is to finger as foot is to toe\n",
    "\n",
    "Word embeddings can help us \"fill in the blank\" for analogies, given just three of the words. The same GloVe function `most_similar()` can be given different parameters to compute analogies (see [documentation](https://rdrr.io/github/psychbruce/PsychWordVec/man/most_similar.html)). Knowing what we know so far about vectors and analogies, complete the definition of the `analogy()` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde08a8-9a04-4cb3-8aa2-d80517ee61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst2 = ['man', 'woman', 'king', 'queen', 'japan', 'tokyo', 'italy', 'rome']\n",
    "# for word in lst2: \n",
    "#     similar_words = model.most_similar(word) # getting the most similar word using GloVe API\n",
    "#                                              # by default, it returns the 10 closest words\n",
    "#     similar_words = [w[0] for w in similar_words]\n",
    "#     print(f\"Similar words for '{word}': {', '.join(similar_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e7bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference from CPSC 330 HW7\n",
    "def analogy(worda, wordb, wordc):\n",
    "    result = model.most_similar(\n",
    "        negative=[\n",
    "            #list of words to subtract, or negative\n",
    "            worda\n",
    "        ], \n",
    "        positive=[\n",
    "            #list of words to add, or positive\n",
    "            wordb, wordc\n",
    "        ]\n",
    "    )\n",
    "    return result[0][0] # wordc + (wordb - worda)\n",
    "\n",
    "# Uncomment when ready to try your function\n",
    "analogy('japan', 'tokyo', 'india')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58980ee2",
   "metadata": {},
   "source": [
    "What about:\n",
    "\n",
    "- man is to doctor as woman is to BLANK?\n",
    "\n",
    "See for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75aa25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use analogy() on this set of words\n",
    "\n",
    "analogy('man', 'doctor', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe93d2-28a3-4c49-b21c-db60a6b63b66",
   "metadata": {},
   "source": [
    "You just got a taste of bias in word embeddings..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d9987f",
   "metadata": {},
   "source": [
    "### Quantifying Bias in Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c2314",
   "metadata": {},
   "source": [
    "Bias in word embeddings gets introduced through bias in the datasets the word embeddings are trained on, and this bias can go on to affect applications that the embeddings are used in. In this assignment, we are going to focus on gendered bias specifically. As our goal is to reduce the bias, we need a way to \"measure\" it, so we will now if our efforts have been successful or not. But how do we quantify gendered bias in word embeddings?\n",
    "\n",
    "There is not a clearcut answer to this, but Bolukbasi et al. in their 2016 [paper](https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf) on debiasing word embeddings recommend calculating the projection of each of the embeddings onto the \"gender direction\", which is the vector difference of the embedding for the word \"he\" and the embedding for the word \"she\". The greater the value of the projection, the more biased the embedding is. Negative projections indicate words more associated with women and positive projections indicate words more associated with men.\n",
    "\n",
    "The picture below represents this concept (although the original he/she vectors are used, instead of their difference): if we look at other words in relation to the vectors \"he\" and \"she\", we will find that some are closer to \"he\", and therefore their projection along this vector is bigger (\"doctor\", in this example), while the opposite is true for female-gendered words (like \"nurse\"). Neutral words should have similar projections among both vectors.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"biasEmbeddings.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "#### A note on Sex and Gender\n",
    "\n",
    "Sex and Gender have different meanings, despite often being used interchangeably. In humans, sex refers to a set of biological features such as chromosomes and gene expression. It is usually characterized as male or females, although intersex attributes are also possible. Gender refers to socially constructed roles, behaviours and identities, such as man, woman, or gender diverse. In this notebook, we use \"man/woman\" or \"male-gendered/female-gendered\" to emphasize the fact that biased word are more often associated with one of these genders than a biological sex. \n",
    "\n",
    "We will use another package for this portion of the exercise, called Natural Language Toolkit ([NLTK](https://www.nltk.org/)) - sorry, word embeddings are complex and some packages are better at some tasks than others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730fa2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadb1131-b1a2-488c-9627-61cc52992949",
   "metadata": {},
   "source": [
    "The code in the cell below does exactly what suggested by Bolukbasi - it takes the difference between the vectors \"he\" and \"she\" and computes the dot product of this vector with each word in the corpus, thus giving us a measure of how biased is that word.\n",
    "\n",
    "**Note:** the original paper includes the normalization of all vectors so that the magnitude of a vector would not influence the measure of bias associated with it. It is, however, a rather lengthy process and, even though it would produce more accurate results, it is not strictly necessary for the purpose of this assignment. For us, it is sufficient to know that the projection of all vectors will have the right sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7841a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each word, we will store the projection in this dictionary\n",
    "projections = {}\n",
    "\n",
    "# He-she direction\n",
    "he_she_direction = model['he'] - model['she']\n",
    "\n",
    "# Dot product between every word and the he-she direction\n",
    "for word in model.key_to_index:\n",
    "    if word in words:\n",
    "        proj_val = np.dot(model[word], he_she_direction)  # projection calculated as dot product\n",
    "        projections[word] = proj_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28824d1",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "In `projections`, look up the word 'student'. What is its projection value? Does this indicate bias toward man or toward woman?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "projections[\"student\"] # -0.565183\n",
    "# projections.get(\"student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef76925-5665-4577-9950-1207420ee49d",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The projection value -0.565183, which is negative, indicating bias towards women.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922cf7fb",
   "metadata": {},
   "source": [
    "Here are the top 30 biased words for each gender, according to the method we described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c47ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Biased toward man\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "proj_dict = dict(sorted(projections.items(), key=lambda x:x[1], reverse=True)[:30])\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.barh(list(proj_dict.keys()), list(proj_dict.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7af99f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Biased toward woman\n",
    "\n",
    "proj_dict = dict(sorted(projections.items(), key=lambda x:-x[1], reverse=True)[:30])\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.barh(list(proj_dict.keys()), list(proj_dict.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb6909-a351-4a7e-9115-c40ea696af2c",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Look carefully at the lists of words. Which words, if any, do you think should remain associated with a specific gender? Which words should not be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd1bc0-736e-4da1-a131-8950a115a330",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The words that are related to gendered names (e.g. \"maria\", \"mary\", \"amy\", \"caroline\"), or are explicitly referring to terms exclusive to certain genders (e.g. \"caliph\", \"ballerina\", \"archduchess\", \"aunt\", \"her\") should remain associated with specific genders. Gender-neutral terminology involving occupations and appearance should not be associated with specific genders (e.g. \"blonde\", \"coach\", \"tactical\", \"ravens\", \"topless\"). There are extremely few words among the top 30 words biased towards men that should remain associated with men, as the majority of the terms are sports, government and military terminology. The majority of the top 30 words biased towards women are terms that should remain associated with women.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb5607",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "We can look at the distribution of projections as a histogram, to get a sense of the typical values. What gender are most words bias toward? If you think the histogram is not sufficiently clear, you may also compute the average bias in the projections dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb7f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sum(projections.values())/len(projections.values()))\n",
    "plt.hist(projections.values(), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95572c8",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Most words are biased toward the female gender, as the histogram is slightly skewed toward the left, and the average bias in the projections dictionary is negative.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babd6c9-abbf-4f7f-8128-d10d7be0483f",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Let's look at the most similar words to some words that appeared as highly biased (*ballerina, amy,* and *league*). Do you notice any word that surprises you, or that you think should not be there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd718e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word in ['ballerina', 'amy', 'league']: \n",
    "    similar_words = model.most_similar(word)\n",
    "    similar_words = [w[0] for w in similar_words]\n",
    "    print(f\"Similar words for '{word}': {', '.join(similar_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba08eb0",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The words \"ingenue\" and \"courtesan\" should not be presented as similar to \"ballerina\", at least in part due to the connotations associated with the terms.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377f26a",
   "metadata": {},
   "source": [
    "The next cell assigns a label to the 1000 most biased words in the dictionary: 1 for male-gendered and -1 female-gendered. These labels will be helpful to see if we can effectively remove the bias from the word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd621057",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dict = dict(sorted(projections.items(), key=lambda x:abs(x[1]), reverse=True)[:1000])\n",
    "ground_truth_lst = [1 if proj_dict[k] >= 0 else -1 for k, v in proj_dict.items()]\n",
    "ground_truth_dct = {k: 1 if proj_dict[k] >= 0 else -1 for k, v in proj_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d1f11-8c31-4305-96db-3715adf80cb6",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "We can use [Principal Component Analysis](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) to visualize the distributions of the 1000 most biased words in a two-dimensional space. \n",
    "\n",
    "The code below produces this plot. Do you think it would be easy for a clustering algorithm to separate the words into two clusters? Do you think these clusters correspond to the two groups of gendered words we are analyzing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81cbd18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "df = pca.fit_transform([model[i] for i in proj_dict])\n",
    "\n",
    "plt.scatter(df[:, 0] , df[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36957928",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">It would be reasonably easy for a clustering algorithm to separate the words into two separate clusters. However, as it is, it is unclear which group corresponds to which gender.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a801311",
   "metadata": {},
   "source": [
    "Running the cell below, we can see that a simple clustering algorithm (K-Means) has no issue separating the two groups, and they correspond with the clusters we could observe after reducing dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95fba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "label = kmeans.fit_predict([model[i] for i in proj_dict])\n",
    "\n",
    "for i in [0, 1]:\n",
    "    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee6c01",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Let's see how well the clusters from K-Means match up with the bias labels we determined from the projections. Comment on the output of this cell. How well was K-Means able to create groups that match the original projections?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fcf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stats\n",
    "\n",
    "neg_cluster = stats.mode([label[idx] for idx, val in enumerate(ground_truth_lst) if val == -1])\n",
    "pos_cluster = stats.mode([label[idx] for idx, val in enumerate(ground_truth_lst) if val == 1])\n",
    "\n",
    "sum([1 for idx, val in enumerate(ground_truth_lst) if (val == 1 and label[idx] == pos_cluster) \n",
    "     or (val == -1 and label[idx] == neg_cluster)])/len(ground_truth_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95385979",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Practically all of the words were labelled correctly. K-Means was able to create groups that match the original projections, without explictly classifying the words.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e67df6",
   "metadata": {},
   "source": [
    "# Neutralize and Equalize\n",
    "\n",
    "Now that we have a better idea of what it means for a word embedding to be biased, and we also have tools to quantify and visualize that bias, we are going to try to improve it. The code below attempts to mathematically remove the bias using the approach described in Bolukbasi's paper, by removing from the word's vector its projection along the he-she direction. Simply run the cells below to debias the word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7608a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell excludes from debiasing words for which being gendered makes sense, such as \"he\"/\"she\", \n",
    "# \"actor\"/\"actress\", or \"policeman\"/\"policewoman\"\n",
    "\n",
    "gender_specific_pairs = [[\"he\", \"she\"], [\"his\", \"hers\"], [\"him\", \"her\"], [\"man\", \"woman\"], [\"men\", \"women\"], [\"husband\", \"wife\"], [\"himself\", \"herself\"], [\"son\", \"daughter\"], [\"father\", \"mother\"], [\"uncle\", \"aunt\"], [\"guy\", \"gal\"], [\"boy\", \"girl\"], [\"king\", \"queen\"], [\"brother\", \"sister\"], [\"female\", \"male\"], [\"gentleman\", \"lady\"], [\"mom\", \"dad\"], [\"actor\", \"actress\"], [\"boyfriend\", \"girlfriend\"], [\"prince\", \"princess\"], [\"sir\", \"madam\"], [\"grandmother\", \"grandfather\"]]\n",
    "\n",
    "for i in model.key_to_index:\n",
    "    if i in words and 'woman' == i[-5:]:\n",
    "        man_version = i[:-5] + \"man\"\n",
    "        if man_version in model.key_to_index:\n",
    "            gender_specific_pairs.append([man_version, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417f7e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function removes the component of a vector u that is in the direction of a vector v. \n",
    "# We will use it to remove the he-she projection from each word in the embedding.\n",
    "def drop(u, v):\n",
    "    return u - v * u.dot(v) / v.dot(v)\n",
    "\n",
    "gsw = sum(gender_specific_pairs, [])\n",
    "\n",
    "# Removing the gender component of words that are not gender-specific from the 1000 most biased words and normalizing\n",
    "for w in proj_dict:\n",
    "    if w not in gsw: # We only want to debias the words that are not gender-specific.\n",
    "        model[w] = drop(model[w], he_she_direction) # Removing the gender component of the vector that represents the word w\n",
    "        model[w] = model[w]/np.linalg.norm(model[w]) # Normalize the new vector (i.e., keep the direction but change length to 1)\n",
    "\n",
    "# Shifting the vectors for words that are gender specific to make sure that \n",
    "# each non-gender-specific word is equidistant from both words in the gender-specific pair\n",
    "for (a, b) in gender_specific_pairs:\n",
    "    model[a] = model[a]/np.linalg.norm(model[a]) # Normalize vector for a\n",
    "    model[b] = model[b]/np.linalg.norm(model[b]) # Normalize vector for b\n",
    "    u = (model[a] + model[b]) / 2 # Find the vector that is exactly in between a and b\n",
    "    y = drop(u, he_she_direction) # Remove the component from that vector that is in the gender direction\n",
    "    z = np.sqrt(1 - np.linalg.norm(y)**2) \n",
    "    if (model[a] - model[b]).dot(he_she_direction) < 0:  \n",
    "        z = -z\n",
    "    model[a] = z * he_she_direction + y # Since the non gender specific words have their gender component removed now,  we are shifting \n",
    "    model[b] = -z * he_she_direction + y # the gendered words in relation to the gender component so they will be equidistant from the non-gendered words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ad304",
   "metadata": {},
   "source": [
    "Following our debiasing strategy, all words (minus the excluded gendered words) are now equally distant from the vectors \"he and \"she\". Observe this on the word \"coach\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108775a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(model['he'] - model['coach'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e24c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(model['she'] - model['coach'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a717018d",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Let's again use PCA and plot the words after debiasing. Can you separate them into clusters now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04575ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "df = pca.fit_transform([model[i] for i in proj_dict if i not in gsw])\n",
    "\n",
    "plt.scatter(df[:, 0] , df[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ef7a1",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">The words are far more difficult to separate into clear-cut clusters in this form.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d390dc",
   "metadata": {},
   "source": [
    "In the plot below, you can observe K-Means' attempt to separate the words in two clusters (using the original high-dimensional space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac09a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "label_debias = kmeans.fit_predict([model[i] for i in proj_dict if i not in gsw])\n",
    "\n",
    "for i in [0, 1]:\n",
    "    plt.scatter(df[label_debias == i , 0] , df[label_debias == i , 1] , label = i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94ffdf",
   "metadata": {},
   "source": [
    "From the plot, it is not clear if K-Means was able to group the words by the original gendered labels. We can see that there are no obvious clusters, but does this mean the bias is gone? Let's check in the next cell...\n",
    "\n",
    "### Question 10\n",
    "\n",
    "1. Let's see how well the groups found by KMeans overlap with the original gendered labels. What percent of the words are clustered \"correctly\" according to the bias? What does this mean in terms of success of our debiasing efforts? \n",
    "2. Based on this results, would you say that looking at the plot of words in 2D is an effective way to estimate the presence of bias in a dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ba54f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ground_truth_not_gsw = [ground_truth_dct[k] for k in ground_truth_dct if k not in gsw]\n",
    "\n",
    "neg_cluster = stats.mode([label_debias[idx] for idx, val in enumerate(ground_truth_not_gsw) if val == -1])\n",
    "pos_cluster = stats.mode([label_debias[idx] for idx, val in enumerate(ground_truth_not_gsw) if val == 1])\n",
    "\n",
    "sum([1 for idx, val in enumerate(ground_truth_not_gsw) if (val == 1 and label_debias[idx] == pos_cluster) \n",
    "     or (val == -1 and label_debias[idx] == neg_cluster)])/len(ground_truth_not_gsw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224cc2b7",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Your answer here</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad92a57-a650-417b-a498-1c654e773399",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Let's review the similar words to *ballerina, amy,* and *league*, which were originally identified as highly biased. Do you see any change? Do you think this is an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810c17c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for word in ['ballerina', 'amy', 'league']: \n",
    "    similar_words = model.most_similar(word)\n",
    "    similar_words = [w[0] for w in similar_words]\n",
    "    print(f\"Similar words for '{word}': {', '.join(similar_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1cb98",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Your answer here</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54e9d8",
   "metadata": {},
   "source": [
    "## Debiasing during training\n",
    "\n",
    "The method that we have worked through is a method to debias embeddings AFTER they have already been created. What if, instead, the training process itself encouraged producing embeddings with less bias? Zhao et al. describe a way to do this with GloVe, **altering the loss function of the model** to focus any gender specific information into the last coordinate of each vector. This way, the resulting vectors can be used without the last coordinate, removing the gender information. \n",
    "\n",
    "Here, we will load the GloVe embeddings debiased during training, and perform clustering to compare the difference between these and the hard debiased vectors that we produced in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4ce96-4762-4cfe-8579-fa0a9504aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "# f = open('../data/gn_glove.vocab', 'r', encoding=\"utf-8\")\n",
    "f = open('/Users/student/Documents – SNG058/Canada - UBC/2024 Term 1/DSCI 430/DSCI-430-Group-2/Module 5/data/gn_glove.vocab', 'r', encoding=\"utf-8\")\n",
    "vocab = [i.strip() for i in f.readlines()]\n",
    "# vecs = np.load('../data/gn_glove.wv.npy')\n",
    "vecs = np.load('/Users/student/Documents – SNG058/Canada - UBC/2024 Term 1/DSCI 430/DSCI-430-Group-2/Module 5/data/gn_glove.wv.npy')\n",
    "gn_dct = {vocab[i]: vecs[i] for i in range(len(vecs))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53666cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "proj_dict_subset = {k: v for k, v in proj_dict.items() if k in gn_dct}\n",
    "label = kmeans.fit_predict([gn_dct[i] for i in proj_dict_subset])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "df = pca.fit_transform([gn_dct[i] for i in proj_dict_subset])\n",
    " \n",
    "for i in [0, 1]:\n",
    "    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb61be",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_bias = [label[idx] for idx, i in enumerate(proj_dict_subset) if proj_dict_subset[i] < 0]\n",
    "male_bias = [label[idx] for idx, i in enumerate(proj_dict_subset) if proj_dict_subset[i] > 0]\n",
    "\n",
    "right_cluster = [1 for i in female_bias if i == stats.mode(female_bias)] + [1 for i in male_bias if i == stats.mode(male_bias)]\n",
    "pct_clustered_by_gender = round(100*sum(right_cluster)/len(proj_dict_subset), 2)\n",
    "print(\"With GloVe embeddings debiased during training, {} percent of terms are clustered according to gender.\".format(pct_clustered_by_gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb11c6",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "Answer the following questions to test your understanding on bias in word embeddings:\n",
    "\n",
    "1. What are the reasons that word embeddings can be biased? How do you think using different data sets to train embeddings might affect the type or amount of bias?\n",
    "2. What are some real world applications that might be affected by the remaining bias in the embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea04c0",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Your answer here</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1341d79-f145-4ec9-9e1c-c478d03f0c70",
   "metadata": {},
   "source": [
    "# Bias in Large Language Models (LLMs)\n",
    "\n",
    "Large Language Models (like ChatGPT) are deep neural networks trained on *very large* corpus of text to be able to produce human-like sentences. Their capabilities are impressive, but since they are typically trained on text found online, they can be rigged with all sorts of biases. Here are a couple of examples (source: https://medium.com/mlearning-ai/inherent-human-bias-in-chat-gpt-ed803d4038fe)\n",
    "\n",
    "Bias based on nationality:\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"chatGPTnationBias.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "Bias based on race and gender: \n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"chatGPTraceBias.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "Here is another example of bias based on gender when making jokes, which I was able to generate on 2023/11/05:\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"jokeBias.png\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "Developers behind these language models are constantly trying to eliminate the problem (as you can imagine, biased responses are not well received by the public). As far as we know (which is limited - many of these models are kept in secret) bias is prevented by better data curation, model fine-tuning, and more strict evaluation. In fairness, LLMs are becoming better and better at avoiding biased responses, but the problem still exists in some forms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed5e72-5fec-4420-9203-59d59b85960e",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "A team at UC Berkeley developed a [tool](https://chat.lmsys.org/?utm_medium=email&utm_source=gamma&utm_campaign=-lmsys-2023) to allow people to test and evaluate LLMs models. You are going to use this tool to test different language models and \"hunt\" for persistent biases.\n",
    "\n",
    "Do the following:\n",
    "1. Go to https://chat.lmsys.org/?utm_medium=email&utm_source=gamma&utm_campaign=-lmsys-2023\n",
    "2. Pick one option:\n",
    "    - Under Arena (side-by-side), pick two models to compare (it can be a random choice) and enter a prompt - remember that your goal is to generate a biased answer. Observe the 2 answers: where you able to find bias in at least one of the models? Is a model more biased than the other?\n",
    "    - Under Direct Chat, pick a model (it can be a random choice) and enter 2-3 prompts, each time changing the question to include a different population (like we saw in the lightbulb jokes example). Compare the answers: can you observe differences caused by the changes introduced in the question?\n",
    "3. Report your findings here, **including your prompt, the names and the answers of both models (the names of the models are visible after their answers), and your evaluation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83d50e-9bd4-4556-b843-d3efb1faece3",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Your answer here</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2a10c-3b8e-4b18-932d-4b06e60f5052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSCI 430",
   "language": "python",
   "name": "dsci430"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
